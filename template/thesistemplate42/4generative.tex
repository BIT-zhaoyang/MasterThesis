\chapter{Scoring Patient Visits by Markov Models}
%MM: mention sequential data
%HMM, advantages: more complex dependence, enable contruction from simpler blocks
\label{chapter:generative}
This chapter is going to explore generative methods for anomaly detection. Generative methods are a collection of algorithms which try to build a model that explains the process of how the data generates. Then, the model gives a score indicating how likely one entry is an anomaly. A family of algorithms belonging to this category is the Markov models~\cite{isaacson1976markov}.

The patients visits can be seen as time sequential data consisting of a series of events. The events in one visit are not generated independently and randomly. Instead, past events have an effect on the type of the next possible event. To handle sequential data, Markov models are good choices since they consider the relation between consecutive observations. In the following context, Section~\ref{sec:MM} introduces the basic Markov chain model. Later, Section~\ref{sec:HMM} expands the Markov chain to a more complicated Hidden Markov Model by introducing hidden variables.

\section{Discrete Markov Process}
\label{sec:MM}
Consider a system having \(K\) distinct states \(\{s_1, s_2, \cdots, s_K\}\). At any time, the system will be in one of these states. After a given time period \(N\), a series observation \(\{x_1, x_2, \cdots, x_N\}\), where $x_n \in \{s_1, s_2, \cdots, s_K\}$, can be obtained. (Without loss of generality, the following discussion assumes the variables are all scalar. The assumption holds in the rest of the context unless explicitly stated otherwise) According to the product rule of probability, the joint probability distribution for this sequence of observations is
\begin{equation}
	p(x_1, x_2, \cdots, x_T) = \prod_{n = 2}^{N} p(x_n \mid x_1, \cdots, x_{n-1})
\end{equation}

The conditional probability distribution of each observation \(x_n\) depends on all observations having a smaller index than it. The above relations between the observations can be represented graphically in Figure~\ref{fig:MM}(a). The graph is fully connected, and no independence property can be obtained from it. Now assume that each observation \(x_n\) only depends on one immediate previous observation \(x_{n-1}\). Then the joint distribution becomes 
\begin{equation}
	p(x_1, x_2, \cdots, x_N) = p(x_1)\prod_{n = 2}^{N} p(x_n \mid x_{n-1})
\end{equation}
This newly obtained model is depicted in Figure~\ref{fig:MM}(b), and is referred as \textit{first-order Markov chain}. The term \textit{first-order} indicates the dependence on only one previous observation. Suppose the system has only 3 states, as shown in Figure~\ref{fig:MM}(c). Then, to fully represent the system, the only required information is the transition probabilities between different sates. The transition probabilities are usually referred as \textit{transition matrix}, denoted as \(\mathbf{A}\). Each element \(A_{ij}\) represents the probability of transferring from state \(s_i\) to state \(s_j\). Learning the parameters of this model is very simple. Since the states are exactly the observations, \(A_{ij}\) can be simply obtained by compute the frequency of transferring to \(s_j\) starting from \(s_i\). The number of free parameters in this model is \(K(K-1)\), where \(K\) represents the number of states in the system.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[scale = 0.75]{images/MM.png}
		\caption{Illustration of a Markov Chain of 4 observations possessing 3 states. Variables are represented using filled circles, while states are represented using filled squares. Variable value can be only from $\{s_1, s_2, s_3\}$.}
		\label{fig:MM}
	\end{center}
\end{figure}

Sometimes, the observations can depend on more than one observations in the past. One simple way to achieve this is creating a higher order Markov chain. By allowing each observation to depend on previous two values, a second-order Markov chain is obtained, as shown in Figure~\ref{fig:secondOrderMM}. Then the joint distribution becomes
\begin{equation}
	p(x_1, x_2, \cdots, x_N) = p(x_1)p(x_2 \mid x_1)\prod_{n = 3}^{N} p(x_n \mid x_{n-1}, x_{n-2})
\end{equation}
Using the same state space representation, the \textit{second-order Markov chain} has better capability of modelling complex relations between variables, compared to \textit{first-order Markov chain}. In fact, the higher the order is, the more flexible the model is. However, the number of parameters grows as well, which makes the model difficult to train. For a \(M^{th}\)-order Markov Chain, there will be \(K^{M}(K-1)\) parameters. Because the exponential growth in number of parameters, the model gradually becomes impractical as \(M\) grows. 

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[scale=0.8]{images/secondOrderMM}
		\caption{Illustration of a second-order Markov model.}
		\label{fig:secondOrderMM}
	\end{center}
\end{figure}


\section{Hidden Markov Model}
\label{sec:HMM}
The simple Markov Chain model is not enough for modelling the patient visit sequence. The variables \(\{x_1, x_2, \cdots, x_N\}\) can be considered as the patient states, namely \texttt{ENROLLING}, etc. However, the visit sequence also contains time part. To integrate the time part into the model, the Markov Chain model can be expanded in another way, by associating an emission distribution \(\mathbf{E}_k\), \(k = 1, \cdots , K\), to each state in the system. Thus, two observations \(x_n\), \(y_n\) exist at any time, where \(y_n\) is generated depending on \(x_n\). If the relation between \(\{x_1, x_2, \cdots, x_N\}\) is modelled as a first-order Markov chain, the joint distribution becomes
\begin{equation}
	p(x_1, y_1, \cdots, x_N, y_N) = p(x_1) \prod_{n = 2}^{N} p(x_n \mid x_{n-1}) \prod_{n = 1}^{N}p(y_n|x_n)
\end{equation}
where \(x_t\) represents the patient state and \(y_t\) represents the associated duration. For each visit, the patient goes through a series of events, which is the patient state. Each event will then last for a certain period of time. The duration can be seen as generated from a distribution, and the parameters of this distribution depend on the event. One example is shown in Figure~\ref{fig:patientMM}. This model is in fact a special case of Hidden Markov Model. This section explores Hidden Markov Model in details.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{images/patientMM}
		\caption{Modelling a patient visit as a special case of Hidden Markov Model.}
		\label{fig:patientMM}
	\end{center}
\end{figure}

\subsection{Definition of Hidden Markov Model}
As mentioned in the last section, a trade-off between flexibility and practicality exists in deciding the order number for a Markov chain model. It would be ideal if a model is not limited to any specific given order, and still only limited number of parameters are required to specify the model. Luckily, these requirements can be satisfied by constructing a Hidden Markov Model using additional latent variables~\cite{PRML}. 

Suppose a sequence of observations \(\mathbf{X} = \{x_1, \cdots, x_N\}\) is obtained. Instead of assuming each observation depends directly on a specific number of previous observations, the new assumption is that, there is a latent variable \(z_t\) corresponding to each observation, and the latent variables form a Markov chain. The latent variables don't have to possess any physical meanings. They can even be of different type to the observations, in terms of distribution and dimensionality. A graphical representation of this model is shown in Figure~\ref{fig:HMM}. It's easy to get confused by comparing Figure~\ref{fig:patientMM} and Figure~\ref{fig:HMM} since they share the same graphical structure. The difference is that, in Figure~\ref{fig:HMM}, the \(z_t\)'s are unobserved latent variables, which is depicted using unfilled circles, while in Figure~\ref{fig:patientMM}, both events and duration are observed values. All observed variables are represented using filled circles. Despite the fact there are no unobserved variables in Figure~\ref{fig:patientMM}, it still belongs to HMM family. In this model, we just happen to observe all variables. It is possible to add additional latent variables into this model. One potential structure could be the one shown in Figure~\ref{fig:patientHMM}. Intuitively, in this model, the value of the newly added latent variable determines which event will generate, then the event determines how long the duration will be. Notice that the latent variables don't have any associated physical meaning or specific distribution form. One can explain them as indication of the functioning status of the system by selecting them to be binary variables. When \(z_t\) = 1, it indicates the queue system in the hospital is working in normal mode. When \(z_t\) = 0, it means the system is working in a problematic way. A similar model has been proposed by Hollm\'en and Tresp~\cite{hollmen2000hidden}.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[scale = 0.8]{images/HMM}
		\caption{Graphical representation of a Hidden Markov Model. Observations are represented using filled circles, while latent variables are depicted using unfilled circles. The latent variables form a first-order Markov chain.}
		\label{fig:HMM}
	\end{center}
\end{figure}

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{images/patientHMM}
		\caption{Modelling patient visit as Hidden Markov Model. Event types and event duration are represented using filled eclipses and circles respectively. Additional hidden variables are represented using unfilled circles. No specific physical meaning is associated with these latent variables.}
		\label{fig:patientHMM}
	\end{center}
\end{figure}

In the framework of HMM, the joint distribution over both observed and latent variables is given below
\begin{equation}
	p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta}) = p(z_1 | \pi) \prod_{n=2}^{N}p(z_n\mid z_{n-1}, \mathbf{A}) \prod_{m=1}^{N}p(x_m\mid z_m, \phi)
	\label{eq:HMMcomplete}
\end{equation}
where \(\mathbf{X} = \{x_1, \cdots, x_N\}\) represents all the observed variables, \(\mathbf{Z} = \{z_1, \cdots, z_N\}\) represents latent variables, and \(\boldsymbol{\theta} = \{\pi, \mathbf{A}, \phi\}\) represents the parameters in this model. The \(\pi\) is a prior distribution for deciding the value of the first variable \(z_1\). The matrix \(\mathbf{A}\) is the transition matrix among the latent variables. The \(\phi\) are the parameters of the emission distribution associated with \(z_t\) and \(x_t\).

\subsection{Learning and Inference}
There are three basic problems in HMM.~\cite{rabiner1989tutorial}  These problems are described below using above notations:
\begin{itemize}
	\item Problem 1: Given a sequence of observations \(\mathbf{X} = \{x_1, \cdots, x_N\}\), what is the probability \(p(\mathbf{X} | \boldsymbol{\theta}) 			  \)over the observations, under specific parameters \(\boldsymbol{\theta} = \{\pi, \mathbf{A}, \phi\}\)?
	\item Problem 2: What's the value of the parameters which maximizes the likelihood \(p(\mathbf{X} | \boldsymbol{\theta})\)?
	\item Problem 3: Given a sequence of observations \(\mathbf{X} = \{x_1, \cdots, x_N\}\), what is the value of the corresponding latent variables?
\end{itemize}
If luxury of observing the latent variable is available, these three problems becomes trivial. But if we do not have this luxury, these three problems become complicated. The rest of the context focus on the first two questions, assuming the latent variable are unobservable. The reason is that, once the value of \(p(\mathbf{X} | \boldsymbol{\theta})\) is computed, the decision on whether a given sequence is anomaly can be made by comparing \(p(\mathbf{X} | \boldsymbol{\theta})\) to a threshold value.

Though it seems more intuitive that finding a way to evaluate \(p(\mathbf{X} | \boldsymbol{\theta})\) should come before maximizing it with respect to the parameters, it would be more convenient to start at solving problem 2. After solving problem 2, the solution to the first problem will appear naturally. The following discussion begins by introducing some new concepts and notations.

The distribution over only observed variables \(p(\mathbf{X} | \boldsymbol{\theta})\) is usually referred as \textit{incomplete likelihood}, while distribution over both observed and unobserved variables \(p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta})\) is referred as \textit{complete likelihood}. Using Equation~\ref{eq:HMMcomplete}, the logarithm of incomplete likelihood can be represented as
\begin{equation}
	\begin{split}
		\ln p(\mathbf{X} | \boldsymbol{\theta}) & = \ln \sum_{\mathbf{Z}}p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta}) \\
										   & = \ln p(z_1 | \pi)  + \ln \sum_{\mathbf{Z}}\Big( \prod_{n=2}^{N}p(z_n\mid z_{n-1}, \mathbf{A}) \prod_{m=1}^{N}p(x_m\mid z_m, \phi)\Big)
	\end{split}
\end{equation}
The above equation is a generalization of the \textit{mixture distribution}~\cite{PRML}. Maximizing $\ln p(\mathbf{X} | \boldsymbol{\theta})$ with respect to the parameters is very difficult since the derivatives don't have a closed form. An alternative practical working algorithm is the \textit{expectation-maximization(EM)} algorithm~\cite{dempster1977maximum}\cite{mclachlan2007algorithm}. The EM algorithm is very similar to the K-Means algorithm mentioned in Chapter~\ref{chapter:clustering}. The algorithm consists of two steps, E-step and M-step. In the E-step, the algorithm fixes the value of parameters and find the posterior distribution of the latent variables \(p(\mathbf{Z} | \mathbf{X}, \boldsymbol{\theta}^{old})\). Here the notation is adopted from Bishop's~\cite{PRML}. The superscription \textit{old} in \(\theta^{old}\) means the parameter is fixed. Then the algorithm computes the expectation of the logarithm of the complete likelihood, with respect to the derived posterior distribution. The newly derived term becomes a function of \(\boldsymbol{\theta}\), which is shown below
\begin{equation}
	Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old}) = \sum_{\mathbf{Z}}p(\mathbf{Z}| \mathbf{X}, \boldsymbol{\theta}^{old})\ln p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta})
\end{equation}
Then in the M-step,  the new value of \(\boldsymbol{\theta}\) is updated by maximizing \(Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old})\). Compared to K-Means, the E-step corresponds to assign each point to a cluster prototype, and the M-step corresponds to update the value of the prototypes. These two steps are executed alternatively until convergence or maximum number of iteration is reached. In the following text, \(\gamma(\mathbf{z}_n)\) and \(\gamma(\mathbf{z}_{n-1}, \mathbf{z}_n)\) are introduced which stand for the posterior distribution of a single latent variable and the joint posterior distribution over two consecutive latent variables, separately. Instead of assuming the latent variables are scalar, here they are represented using \textit{1-of-K} coding. Namely, each latent variable is a length \(K\) vector, where one and only one of these \(K\) elements equals 1. When \(z_{nk} = 1\), it means the \(nth\) latent variable is in the \(kth\) state. Using this representation schema, following equations are obtained~\cite{PRML}
\begin{align}
	\gamma(\mathbf{z}_n) = &p(\mathbf{z}_n | \mathbf{X}, \boldsymbol{\theta}^{old})	\\
	\gamma(\mathbf{z}_{n-1}, \mathbf{z}_n) = &p(\mathbf{z}_{n-1}, \mathbf{z}_n | \mathbf{X}, \boldsymbol{\theta}^{old}) \\
	Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{old}) = &\sum_{k=1}^{K} \gamma(z_{1k})\ln \pi_k + \sum_{n=2}^{N}\sum_{j=1}^{K}\sum_{k=1}^{K}\gamma(\mathbf{z}_{n-1}, \mathbf{z}_n)\ln A_{jk}		\notag		\\
		&+ \sum_{n=1}^{N}\sum_{k=1}^{K}\gamma(z_{nk})\ln p(x_n|\phi_k)	
\end{align}

Computation in the M-step is relatively easy. Assume the E-step has been done, so that  \(\gamma(\mathbf{z}_n)\) and \(\gamma(\mathbf{z}_{n-1}, \mathbf{z}_n)\) are like constants now. Then following update equation can be obtained
\begin{align}
	\pi_k = & \frac{\gamma(z_{1k})}{\sum_{j=1}^{K}\gamma(z_{1j})}	\\
	A_{jk} = & \frac{\sum_{n=2}^{N}\gamma(z_{n-1,j},z_{nk})}{\sum_{l=1}^{K}\sum_{n=2}^{N}\gamma(z_{n-1,j},z_{nl})}
\end{align}
Update of \(\phi_k\) is more tricky, since it depends on the specific choice of the emission distribution. One good observation is that, only the final term depends on \(\phi_k\), and different \(\phi_k\) doesn't couple with each other. Thus, each \(\phi_k\) can be updated separately. The term \(\gamma(z_{nk})\) functions as a soft assignment, representing the probability of assigning a point \(x_n\) to each state.

Computation in E-step is more difficult which requires efficient algorithm. The most widely used algorithm is known as \textit{alpha-beta} algorithm. This algorithm can be seen as an application of dynamic programming technique which takes advantage of the tree structure in HMM thus leading to efficiency. To start with the alpha-beta algorithm, following conditional independence properties should be obtained first~\cite{jordan2003introduction}
\begin{align}
	p(\mathbf{X}| \mathbf{z}_n) = &p(x_1, \cdots, x_n | \mathbf{z}_n)	\notag \\	
										&p(x_{n+1}, \cdots, x_N | \mathbf{z}_n) \\
	p(x_1, \cdots, x_{n-1}| x_n, \mathbf{z}_n) = &p(x_1, \cdots, x_{n-1} | \mathbf{z}_n)	\\
	p(x_1, \cdots, x_{n-1}| z_{n-1}, \mathbf{z}_n) = &p(x_1, \cdots, x_{n-1} | \mathbf{z}_{n-1})
\end{align}
These equations can be obtained by using \textit{d-seperation} technique~\cite{pearl2014probabilistic}, or proved formally using sum and product rules of probability. Using the first independence property and Bayes' theorem, following equations are obtained
\begin{equation}
	\label{eq:gammaZn}
	\begin{split}
	\gamma(\mathbf{z}_n) &= p(\mathbf{z}_n | \mathbf{X})  = \frac{p(\mathbf{X}| \mathbf{z}_n)p(\mathbf{z}_n)}{p(\mathbf{X})}	\\
										 & = \frac{p(x_1, \cdots, x_n, \mathbf{z}_n)p(x_{n+1}, \cdots, x_N | \mathbf{z}_n)}{p(\mathbf{X})}	\\
										 & = \frac{\alpha(\mathbf{z}_n)\beta(\mathbf{z}_n)}{p(\mathbf{X})}
	\end{split}
\end{equation}
where
\begin{align}
	\alpha(\mathbf{z}_n) & = p(x_1, \cdots, x_n, \mathbf{z}_n)\\
	\beta(\mathbf{z}_n)  & = p(x_{n+1}, \cdots, x_N | \mathbf{z}_n)
\end{align}
Using the other two conditional independence properties, \(\alpha(\mathbf{z}_n)\) can be expressed recursively in terms of \(\alpha(\mathbf{z}_{n-1})\)
\begin{equation}
	\begin{split}
	\alpha(\mathbf{z}_n) &=  p(x_n | \mathbf{z}_n) \sum_{\mathbf{z}_{n-1}}p(x_1, \cdots, x_{n-1}, \mathbf{z}_{n-1})p(\mathbf{z}_n | \mathbf{z}_{n-1}) \\
						 &=  p(x_n | \mathbf{z}_n) \sum_{\mathbf{z}_{n-1}} \alpha(\mathbf{z}_{n-1})p(\mathbf{z}_n | \mathbf{z}_{n-1})
	\end{split}
\end{equation}
Similarly, \(\beta(\mathbf{z}_n)\) can also be expressed recursively as
\begin{equation}
	\beta(\mathbf{z}_n) = \sum_{\mathbf{z}_{n+1}}\beta(\mathbf{z}_{n+1})p(x_{n+1}|\mathbf{z}_{n+1})p(\mathbf{z}_{n+1}|\mathbf{z}_n)
\end{equation}
The term \(\alpha(\mathbf{z}_n)\) can be seen as messages propagated from the beginning to the end. Each \(\alpha(\mathbf{z}_n)\) receives messages passed from its predecessor, combines these information with its own information and then pass them to its successor. The logical also applies to the term \(\beta(\mathbf{z}_n)\), but the messages are from the end to the beginning. Due to the tree structure in HMM, computing each term only depends on one adjacent term, instead of all terms before/after it. Thus, the computation reduces dramatically which makes the algorithm efficient. To start the whole computation, initial conditions \(\alpha(\mathbf{z}_1)\) and \(\beta(\mathbf{z}_n)\) are required. The initial conditions are given below
\begin{align}
	\alpha(\mathbf{z}_1) &= \prod_{k=1}^{K}\{\pi_k p(x_1 | \phi_k) \}^{z_{1k}}	\\
	\beta(\mathbf{z}_N) &= 1
\end{align}

Having obtained \(\alpha(\mathbf{z}_n)\) and \(\beta(\mathbf{z}_n)\), the posterior distribution \(\gamma(\mathbf{z}_n)\) can be computed as in equation~(\ref{eq:gammaZn}). As for \(\gamma(\mathbf{z}_{n-1}, \mathbf{z}_n)\), it can be computed as following
\begin{align}
\gamma(\mathbf{z}_{n-1}, \mathbf{z}_n) &= p(\mathbf{z}_{n-1}, \mathbf{z}_n | \mathbf{X}) \nonumber \\
									   & = \frac{p(\mathbf{X} | \mathbf{z}_{n-1}, \mathbf{z}_n)p(\mathbf{z}_{n-1}, \mathbf{z}_n)}{p(\mathbf{X})}\nonumber \\
									   & = \frac{p(x_1, \cdots, x_{n-1} | \mathbf{z}_{n-1}) p(x_n | \mathbf{z}_n) p(x_{n+1}, \cdots, x_N | \mathbf{z}_n)
									   			 p(\mathbf{z}_n | \mathbf{z}_{n-1}) p(\mathbf{z}_{n-1})}{p(\mathbf{X})}\nonumber \\
									   & = \frac{\alpha(\mathbf{z}_{n-1}p(x_n|\mathbf{z}_n)p(\mathbf{z}_n|\mathbf{z}_{n-1})\beta(\mathbf{z}_n)}{p(\mathbf{X})}
\end{align}
Up till now, both steps in EM algorithm are introduced, and the problem 2 can be solved efficiently. The left question is how to solve problem 1, computing the likelihood over the incomplete data. The solution comes from  Equation~\ref{eq:gammaZn}. Notice that \(\gamma(\mathbf{z}_n)\) is a posterior distribution. Integrating both sides of Equation~\ref{eq:gammaZn} over \(\mathbf{z}_n\) gives
\begin{equation}
	p(\mathbf{X}) = \sum_{\mathbf{z}_n} \alpha(\mathbf{z}_n)\beta(\mathbf{z}_n)
\end{equation}
where \(\mathbf{z}_n\) is an arbitrary latent variable. If \(n = N\), then \(\beta(\mathbf{z}_n) = 1\), which makes the above equation simpler
\begin{equation}
	p(\mathbf{X}) = \sum_{\mathbf{z}_N} \alpha(\mathbf{z}_N)
\end{equation}
Then, both problem 1 and problem 2 are solved.