\chapter{Generative Methods}
%MM: mention sequential data
%HMM, advantages: more complex dependence, enable contruction from simpler blocks
\label{chapter:generative}
This chapter is going to explore generative methods for anomaly detection. Generative methods are a collection of algorithms which try to build a model that explains the process of how the data generates. Then, based on information from the proposed model, anomalies are detected. A family of algorithms belonging to this category is the Markov models.

As described in Chapter~\ref{chapter:background}, the patients visits can be seen as time sequential data consisting of a series of events. The events in one visit are not generated independently and randomly. Instead, past events have an effect on the type of the next possible event. To handle sequential data, Markov models are the correct choice since they consider the relation between consecutive observations. In the following context, Section~\ref{sec:MM} introduces the basic Markov chain model. Later, Section~\ref{sec:HMM} expands the Markov chain to a more complicated Hidden Markov Model by introducing hidden variables.

\section{Discrete Markov Process}
\label{sec:MM}
Consider a system having \(N\) distinct states \(S_1, S_2, \cdots, X_N\). At any time, the system will be in one of these states. After a given time period \(t\), a series obeservation \(x_1, x_2, \cdots, x_t\) can be obtained. (Without loss of generality, the discussion assumes the variables are all scalar. The assumption holds in the rest of the context unless explicitly stated otherwise) According to the prodcut rule of probability, the joint distribution for this sequence of observation is
\begin{equation}
	p(x_1, x_2, \cdots, x_t) = \prod_{i = 2}^{t} p(x_i \mid x_1, \cdots, x_{i-1})
\end{equation}

The above relations between the observations can be represented graphically in Fig~\ref{fig:MM}(a). The conditional probability distribution of an observation \(x_i\) depends on all observation having a smaller index than it. The graph is fully connected, and no independence property can be obtained from it. Now assume that each observation \(x_i\) only depends on one immediate previous observation \(x_{i-1}\). Then the joint distribution is 
\begin{equation}
	p(x_1, x_2, \cdots, x_t) = p(x_1)\prod_{i = 2}^{t} p(x_i \mid x_{i-1})
\end{equation}
The obtained model is called \textit{first-order Markov chain}, which is depicted in Fig~\ref{fig:MM}(b). The term \textit{first-order} indicates the dependence on only one previous observation. As an example, suppose the system has only 3 states, as shown in Fig~\ref{fig:MM}(c). Then, to fully represent the system, the only required information is the transition probabilities between different sates. The transition probabilities are usually refered as \textit{transition matrix}, denoted as \(\mathbf{A}\). Each element \(A_{ij}\) represents the probability of transfering to state \(s_j\) from state \(s_i\). Learning the parameters of this model is very simple. Since the observations are the states, \(A_{ij}\) can be simply obatained by compute the frequency of transfering to \(s_j\) starting from \(s_i\). The number of free parameters is \(K(K-1)\), where \(K\) represents the number of states in the system.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[scale = 0.7]{images/MM.png}
		\caption{Illustration of a Markov Chain of 4 observations possessing 3 states. Variables are represented using filled circles, while states are represented using filled squares.}
		\label{fig:MM}
	\end{center}
\end{figure}

Sometimes, the observations can depend on more than one observations in the past. One simple way to achieve this is creating a higer order Markov chain. By allowing the observation to depend on previous two values, a second-order Markov chain is obtained, as represented in Fig~\ref{fig:secondOrderMM}. Then the joint distribution becomes
\begin{equation}
	p(x_1, x_2, \cdots, x_t) = p(x_1)p(x_2 \mid x_1)\prod_{i = 3}^{t} p(x_i \mid x_{i-1}, x_{i-2})
\end{equation}
The higher the order is, the more flexible the model is. However, the complexity of the model increases as well, since the number of parameters grows. For a \(M^{th}\) order markov chain, there will be \(K^{M}(K-1)\) parameters. Because the exponential growth in number of parameters, the model gardually becomes impractial. 

The markov chain model can be also expanded by associating an emission distribution \(\mathbf{E}_k\), \(k = 1, \cdots , K\), to each state in the system. Thus, two observations \(x_i\), \(y_i\) exist at any time, where \(y_i\) is generated depending on \(x_i\). If the expansion is based on a first-order Markov chain, the joint distribution becomes
\begin{equation}
	p(x_1, y_1, \cdots, x_t, y_t) = p(x_1)\big[\prod_{i = 2}^{t} p(x_i \mid x_{i-1})\big]\big[\prod_{i=1}^{t}p(y_i|x_i)\big]
\end{equation}
The patient visits can be seen as an example of the expanded model. For each visit, the patient will go through a series of events, which can be considered as states. Each event will then last for a certain period of time. The duration can be seen as generated from a distribution, and the parameters of this distribution depend on the event. The example is shown in Fig~\ref{fig:patientMM}.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[scale=0.8]{images/secondOrderMM}
		\caption{Illustration of a second-order Markov model.}
		\label{fig:secondOrderMM}
	\end{center}
\end{figure}

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{images/patientMM}
		\caption{Modelling a patient visit as an expansion of first-order Markov model.}
		\label{fig:patientMM}
	\end{center}
\end{figure}


\section{Hidden Markov Models}
\label{sec:HMM}
As mentioned in last section, a tradeoff between flexibility and practicality exists in deciding the order number for a Markov chain model. However, in special situations, such as speech recognition, very high-order Markov chain is expected. Ideally, it would be better to build a general model which is not limited to any specific given order, and still only limited number of parameters are required to specify the model. These requirements can be satisfied by constructing a Hidden Markov Model using additional latent variables. 

Suppose a sequence of observations \(\mathbf{X} = \{x_1, \cdots, x_N\}\) is obtained. In stead of assuming each observation depends directly on a specific number of previous observations, the new assumption is that, there is a latent variable \(z_i\) corresponding to each observation, and the latent variables form a Markov chain. The latent variables don't have to possess physical meanings. They can even be of different type in terms of distribution, dimensionality, to the observations. A graphical representation of this model is shown in Fig~\ref{fig:HMM}. It's easy to get confused by comparing Fig~\ref{fig:patientMM} and Fig~\ref{fig:HMM} since they share the same graphical structure. The difference is that, in Fig~\ref{fig:HMM}, the \(z_i\)'s are unobserved latent variables. This is depicted using unfilled circles. While in Fig~\ref{fig:patientMM}, the events are also observed values as the time duration is. Observed variables are all represented using filled circles. If the HMM is applied to the patient visits, one potential structure could be the one shown in Fig~\ref{fig:patientHMM}. In this model, the value of latent variable determines which event will generate. Notice that the latent variables don't have associated physical meaning and specific distribution form. One can explain them as indication of the functioning statu of the system by selecting them to be binary variables. When \(z_i\) = 1, it indicates the system is working in normal mode. When \(z_i\) = 0, it means the system is working in a problametic way.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[scale = 0.8]{images/HMM}
		\caption{Graphical representation of a Hidden Markov Model. Observations are represented using filled circles, whilc latent variables are depicted using unfilled circles. The latent variables form a first-order Markov chain.}
		\label{fig:HMM}
	\end{center}
\end{figure}

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{images/patientHMM}
		\caption{Modelling patient visit as Hidden Markov Model. Event types and event duration are represented using filled eclipses and circles respectively. Additional hidden variables are represented using unfilled circles. No specific physical meaning is associated with these latent variables.}
		\label{fig:patientHMM}
	\end{center}
\end{figure}

Using the HMM, the joint distribution over both observed and latent variables is given below
\begin{equation}
	p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta}) = p(z_1 \mid \pi) \big[\prod_{n=2}^{N}p(z_n\mid z_{n-1}, \mathbf{A})\big] \big[\prod_{m=1}^{N}p(x_m\mid z_m, \phi)\big]
\end{equation}
where \(\mathbf{X} = \{x_1, \cdots, x_N\}\) represents all the observed variables, \(\mathbf{Z} = \{z_1, \cdots, z_N\}\) represents latent variables, and \(\boldsymbol{\theta} = \{\pi, \mathbf{A}, \phi\}\) represents the parameters in this model. The matrix \(\mathbf{A}\) is the transition matrix among the latent variables. The \(\pi\) is a prior distribution for deciding the value of the first variable \(z_1\). The \(\phi\) are the parameters of the emission distribution associated with \(z_i\) and \(x_i\).

Given above notations, the problem is, given a sequence of observations, what is the value of latent variables, and what is the value of the parameters.

%You have now stated your problem, and you are ready to do something
%about it!  \emph{How} are you going to do that? What methods do you
%use?  You also need to review existing literature to justify your
%choices, meaning that why you have chosen the method to be applied in
%your work.
%
%% An example of a traditional LaTeX table
%% ------------------------------------------------------------------
%% A note on underfull/overfull table cells and tables:
%% ------------------------------------------------------------------
%% In professional typography, the width of the text in a page is always a lot
%% less than the width of the page. If you are accustomed to the (too wide) text
%% areas used in Microsoft Word's standard documents, the width of the text in
%% this thesis layout may suprise you. However, text in a book needs wide
%% margins. Narrow text is easier to read and looks nicer. Longer lines are 
%% hard to read, because the start of the next line is harder to locate when
%% moving from line to the next. 
%% However, tables that are in the middle of the text often would require a wider
%% area. By default, LaTeX will complain if you create too wide tables with
%% ``overfull'' error messages, and the table will not be positioned properly
%% (not centered). If at all possible, try to make the table narrow enough so
%% that it fits to the same space as the text (total width = \textwidth).
%% If you do need more space, you can either
%% 1) ignore the LaTeX warnings 
%% 2) use the textpos-package to manually position the table (read the package
%%    documentation)
%% 3) if you have the table as a PDF document (of correct size, A4), you can use
%%    the pdfpages package to include the page. This overrides the margin
%%    settings for this page and LaTeX will not complain.
%% ------------------------------------------------------------------
%% Another note:
%% ------------------------------------------------------------------
%% If your table fits to \textwidth, but the cells are so narrow that the text
%% in p{..}-formatted cells does not flow nicely (you get underfull warnings 
%% because LaTeX tries to justify the text in the cells) you can manually set
%% the text to unjustified by using the \raggedright command for each cell 
%% that you do not want to be justified (see the example below). \raggedleft 
%% is also possible, of course...
%% ------------------------------------------------------------------
%% If you need to have linefeeds (\\) inside a cell, you must create a new
%% paragraph-formatting environment inside the cell. Most common ones are 
%% the minipage-environment and the \parbox command (see LaTeX documentation
%% for details; or just google for ``LaTeX minipage'' and ``LaTeX parbox'').
%\begin{table}
%\begin{tabular}{|p{2cm}|p{3.8cm}|p{4.5cm}|p{1.1cm}|} 
%% Alignment of sells: l=left, c=center, r=right. 
%% If you want wrapping lines, use p{width} exact cell widths.
%% If you want vertical lines between columns, write | above between the letters
%% Horizontal lines are generated with the \hline command:
%\hline % The line on top of the table
%\textbf{Code} & \textbf{Name} & \textbf{Methods} & \textbf{Area} \\ 
%\hline 
%% Place a & between the columns
%% In the end of the line, use two backslashes \\ to break the line,
%% then place a \hline to make a horizontal line below the row 
%T-110.6130 & Systems Engineering for Data Communications
%    Software & \raggedright Computer simulations, mathematical modeling,
%  experimental research, data analysis, and network service business
%  research methods, (agile method) & T-110 \\ 
%\hline
%\multicolumn{2}{|p{6.25cm}|}{Mat-2.3170 Simulation (here is an example of
% multicolumn for tables)}& Details of how to build simulations & T-110 \\
%% The multicolumn command takes the following 3 arguments: 
%% the number of cells to merge, the cell formatting for the new cell, and the
%% contents of the cell
%\hline
%S-38.3184 & Network Traffic Measurements and Analysis 
%& \raggedright How to measure and analyse network
%  traffic & T-110 \\ \hline
%\end{tabular} % for really simple tables, you can just use tabular
%% You can place the caption either below (like here) or above the table
%\caption{Research methodology courses}
%% Place the label just after the caption to make the link work
%\label{table:courses}
%\end{table} % table makes a floating object with a title
%
%If you have not yet done any (real) metholodogical courses (but chosen
%introduction courses of different areas that are listed in the
%methodological courses list), now is the time to do so or at least
%check through material of suitable methodological courses. Good
%methodologial courses that consentrates especially to methods are
%presented in Table~\ref{table:courses}. Remember to explain the
%content of the tables (as with figures). In the table, the last column
%gives the research area where the methods are often used. Here we used
%table to give an example of tables. Abbreviations and Acronyms is also
%a long table. The difference is that longtables can continue to next
%page.
%
